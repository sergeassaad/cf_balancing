{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.where(x >= 0, \n",
    "                    1 / (1 + np.exp(-x)), \n",
    "                    np.exp(x) / (1 + np.exp(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy data generating mechanism:\n",
    "\n",
    "Fix $\\sigma,\\rho \\in \\mathbb{R}$ and $p\\in \\mathbb{N}$.\n",
    "\n",
    "Fix $\\beta_0,\\beta_\\tau \\in \\mathbb{R}^p$, such that $||\\beta_0||_0 = ||\\beta_\\tau||_0 = p^* <<p$, and $supp(\\beta_0)=supp(\\beta_\\tau)$\n",
    "\n",
    "Fix $\\gamma \\in \\mathbb{R}^p$ such that $||\\gamma||_0= p^* << p$\n",
    "\n",
    "\n",
    "Draw $X_i,T_i, Y_i$ as follows:\n",
    "\n",
    "$$ X_i \\sim \\mathcal{MVN}(0,\\sigma^2 [(1-\\rho)I_p + \\rho 1_p1_p^T])$$\n",
    "\n",
    "$$ P(T_i=1|X_i) = \\sigma(X\\gamma)$$\n",
    "\n",
    "$$\\epsilon_i \\sim \\mathcal{N}(0,1)$$\n",
    "$$Y_i(0) = X_i\\beta_0 + \\epsilon_i $$\n",
    "$$Y_i(1) = Y_i(0) + X_i\\beta_\\tau + \\epsilon_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $||\\gamma||_0 = 0$, then $P(X_i|T_i=0)=P(X_i|T_i=1)$. In this case, the propensity score estimator will perform poorly, and a propensity-based weighted regression should perform worse than/equal to an unweighted regression.\n",
    "\n",
    "As $||\\gamma||_0$ increases, the distance between the distributions $P(X_i|T_i=0)$ and $P(X_i|T_i=1)$ will increase, and so too should the benefit of using a propensity weighted regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def high_dim_simu(p=3000, n=1500, gamma_val=1, rho=0.3, var=2, k=30, h_degree=0.1,true_ate=0,scenario=\"A\", n_copy=100,seed=123):\n",
    "    np.random.seed(seed)\n",
    "    x = np.zeros([n, p, n_copy])\n",
    "    t = np.zeros([n, n_copy])\n",
    "    yf = np.zeros([n, n_copy])\n",
    "    ycf = np.zeros([n, n_copy])\n",
    "    mu1 = np.zeros([n, n_copy])\n",
    "    mu0 = np.zeros([n, n_copy])\n",
    "\n",
    "    # Can change these to random normal if needed\n",
    "    first_half = np.ones(int(np.ceil(float(k) / 2))) # ceil and floor to deal with odd k\n",
    "    second_half = np.ones(int(np.floor(float(k) / 2)))\n",
    "    \n",
    "    first_half_tau = np.ones(int(np.ceil(float(k) / 2)))\n",
    "    second_half_tau = np.ones(int(np.floor(float(k) / 2)))\n",
    "    \n",
    "    \n",
    "    first_half_gamma = gamma_val*np.ones(int(np.ceil(float(k) / 2)))\n",
    "    second_half_gamma = gamma_val*np.ones(int(np.floor(float(k) / 2)))\n",
    "\n",
    "    for i in range(n_copy):\n",
    "        mean_vec = np.zeros(p)\n",
    "        if scenario == \"A\":\n",
    "            beta = np.concatenate((first_half, second_half, np.zeros(p - k)))\n",
    "            beta_tau = h_degree*np.concatenate((first_half_tau, second_half_tau, np.zeros(p - k)))\n",
    "            gamma = np.concatenate((first_half_gamma, second_half_gamma, np.zeros(p - k)))\n",
    "        elif scenario == \"B\":\n",
    "            beta = np.concatenate((first_half, second_half, np.zeros(p - k)))\n",
    "            beta_tau = h_degree*np.concatenate((first_half_tau, second_half_tau, np.zeros(p - k)))\n",
    "#             print(k,p)\n",
    "#             print(type(k//2),type(p-k-k//2))\n",
    "            gamma = np.concatenate((first_half_gamma, np.zeros(k//2), second_half_gamma, np.zeros(p - k - k//2)))\n",
    "        else:\n",
    "            beta = np.concatenate((np.zeros(p - k), first_half, second_half))\n",
    "            beta_tau = h_degree*np.concatenate((np.zeros(p - k), first_half_tau, second_half_tau))\n",
    "            gamma = np.concatenate((first_half_gamma, second_half_gamma, np.zeros(p - k)))\n",
    "\n",
    "        Sigma_x = (np.ones([p, p]) * rho + np.identity(p) * (1 - rho)) * var\n",
    "\n",
    "        x[:, :, i] = np.random.multivariate_normal(mean_vec, Sigma_x, n)\n",
    "        prob_t = sigmoid(x[:, :, i].dot(gamma))\n",
    "        t[:, i] = np.random.binomial(1, prob_t, size=n)\n",
    "\n",
    "        mu0[:, i] = np.matmul(x[:, :, i], beta)\n",
    "        mu1[:, i] = mu0[:, i] + np.matmul(x[:, :, i], beta_tau)+true_ate\n",
    "        if i == 0:\n",
    "            ate = np.mean(mu1[:, i] - mu0[:, i])\n",
    "            t_id = np.where(t[:, i] == 1)\n",
    "            att = np.mean(mu1[t_id, i] - mu0[t_id, i])\n",
    "        noise = np.random.normal(size=n)\n",
    "        yf[:, i] = t[:, i] * mu1[:, i] + (1 - t[:, i]) * mu0[:, i] + noise\n",
    "        ycf[:, i] = t[:, i] * mu0[:, i] + (1 - t[:, i]) * mu1[:, i] + noise\n",
    "        if i%10==0:\n",
    "            print (i, \"th finished\")\n",
    "    \n",
    "    \n",
    "    return {'x': x, 't': t, 'mu0': mu0, 'mu1': mu1, 'yf': yf, 'ycf': ycf, 'ate': true_ate, 'att': att}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the train/test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_custom(data,test_size=0.25,seed=123):\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    n_copy = data['x'].shape[2]\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=123)\n",
    "    train,test = next(sss.split(data['x'][:,:,0],data['t'][:,0]))\n",
    "    \n",
    "    # Initialize variables\n",
    "    data_train={}\n",
    "    data_test={}\n",
    "    data_train['x'] = np.zeros((len(train),data['x'].shape[1],data['x'].shape[2]))\n",
    "    data_test['x'] = np.zeros((len(test),data['x'].shape[1],data['x'].shape[2]))\n",
    "    \n",
    "    for var in ['t','mu0','mu1','yf','ycf']:\n",
    "        data_train[var] = np.zeros((len(train),data[var].shape[1]))\n",
    "        data_test[var] = np.zeros((len(test),data[var].shape[1]))\n",
    "    \n",
    "    # Loop over repetitions\n",
    "    for i in range(n_copy):\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "        train,test = next(sss.split(data['x'][:,:,i],data['t'][:,i]))\n",
    "        \n",
    "        data_train['x'][:,:,i] = data['x'][train,:,i]\n",
    "        data_test['x'][:,:,i] = data['x'][test,:,i]\n",
    "        \n",
    "        for var in ['t','mu0','mu1','yf','ycf']:\n",
    "            data_train[var][:,i] = data[var][train,i]\n",
    "            data_test[var][:,i] = data[var][test,i]\n",
    "        \n",
    "        if(i==0):\n",
    "            \n",
    "            t_id_train = np.where(data_train['t']==1)\n",
    "            t_id_test = np.where(data_test['t']==1)\n",
    "            data_train['att'] = np.mean(data_train['mu1'][t_id_train,i]-data_train['mu0'][t_id_train,i])\n",
    "            data_test['att'] = np.mean(data_test['mu1'][t_id_test,i]-data_test['mu0'][t_id_test,i])\n",
    "        \n",
    "    data_train['ate'] = data['ate']\n",
    "    data_test['ate'] = data['ate']\n",
    "            \n",
    "    return data_train,data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th finished\n",
      "10 th finished\n"
     ]
    }
   ],
   "source": [
    "p=25 # data dimension\n",
    "p_star = 10 #number of dimensions determining outcomes & treatment\n",
    "n=1000 # number of data points\n",
    "n_copy= 20 # number of repetitions\n",
    "rho = 0.3 # correlation between features\n",
    "var = 2 # variance of feature vectors\n",
    "gamma_val = 5\n",
    "SNR = 1 #Signal-to-noise ratio\n",
    "var = SNR/p_star # variance of feature vectors\n",
    "sc = 'A'\n",
    "h_degree=0.3\n",
    "tau=3\n",
    "\n",
    "data = high_dim_simu(p=p, n=n, rho=rho, gamma_val = gamma_val,var=var,k=p_star, h_degree=h_degree, true_ate=tau, scenario=sc, n_copy=n_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train,data_test = train_test_split_custom(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['ate'],data_test['ate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.318144053791723, 3.1168331847548933)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['att'],data_test['att']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/serge/Documents/causal/Causal_IPW/codebase'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "gammas = [0,0.5,1,1.5,3,3.5,5,10,15,20]\n",
    "scenarios = ['A','B','C']\n",
    "for gamma_val in gammas:\n",
    "    for sc in scenarios:\n",
    "           os.mkdir('../datasets/toy_data/{:s}_{:.1f}'.format(sc,gamma_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n",
      "0 th finished\n",
      "10 th finished\n"
     ]
    }
   ],
   "source": [
    "p=25 # data dimension\n",
    "p_star = 10 #number of dimensions determining outcomes & treatment\n",
    "n=1000 # number of data points\n",
    "n_copy= 20 # number of repetitions\n",
    "rho = 0.3 # correlation between features\n",
    "SNR = 1 #Signal-to-noise ratio\n",
    "var = SNR/p_star # variance of feature vectors\n",
    "h_degree=0.3 # size of beta_tau\n",
    "tau=3 # true ATE\n",
    "\n",
    "gammas = [0.0,0.5,1.0,1.5,3.0,3.5,5.0,10.0,15.0,20.0]\n",
    "scenarios = ['A','B','C']\n",
    "for gamma_val in gammas:\n",
    "    for sc in scenarios:\n",
    "        data = high_dim_simu(p=p, n=n, rho=rho, gamma_val = gamma_val,var=var, k=p_star, h_degree=h_degree, true_ate=tau, scenario=sc, n_copy=n_copy)\n",
    "        data_train,data_test = train_test_split_custom(data,test_size=0.25,seed=123)\n",
    "        np.savez('../datasets/toy_data/{:s}_{:.1f}/data_train.npz'.format(sc,gamma_val),**data_train)\n",
    "        np.savez('../datasets/toy_data/{:s}_{:.1f}/data_test.npz'.format(sc,gamma_val),**data_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
